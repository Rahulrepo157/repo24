{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQjNKsWnceB8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, QuantileTransformer, PowerTransformer\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer, explained_variance_score, max_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.stats import boxcox, yeojohnson, skew, pearsonr, spearmanr, kendalltau\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.optimizers import Adam, AdamW\n",
        "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.layers import LayerNormalization, Dropout\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "from keras_tuner import HyperParameters\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# 1. Enhanced Data Generation\n",
        "def generate_data(num_samples=3000, temporal=False):\n",
        "\n",
        "    data = {\n",
        "        'Engine_Speed_RPM': np.random.randint(1000, 3000, num_samples),\n",
        "        'Load_Percentage': np.random.uniform(0, 100, num_samples),\n",
        "        'Injection_Timing_Deg': np.random.uniform(5, 20, num_samples),\n",
        "        'Compression_Ratio': np.random.uniform(14, 20, num_samples),\n",
        "        'Fuel_Type': np.random.choice(['Diesel', 'BioDiesel_20', 'BioDiesel_40'], num_samples, p=[0.5, 0.3, 0.2]),\n",
        "        'Ambient_Temperature_C': np.random.uniform(20, 40, num_samples),\n",
        "        'Air_Fuel_Ratio': np.random.uniform(14, 18, num_samples),\n",
        "        'Intake_Air_Pressure_kPa': np.random.uniform(90, 110, num_samples),\n",
        "        'Coolant_Temperature_C': np.random.uniform(70, 90, num_samples)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Create temporal effects\n",
        "    if temporal:\n",
        "        time = np.arange(num_samples) / num_samples * 10  # Time in arbitrary unit\n",
        "        df['Engine_Speed_RPM'] += (np.sin(time) * 50).astype(int)\n",
        "        df['Ambient_Temperature_C'] += (np.cos(time) * 5).astype(float)\n",
        "\n",
        "    # Simulate efficiency with interactions and non-linearity\n",
        "    df['BioThermal_Efficiency'] = 40 + 0.02 * df['Engine_Speed_RPM'] + 0.15 * df['Load_Percentage'] - 0.8*df['Injection_Timing_Deg'] + 0.3 * df['Compression_Ratio'] - 0.05 * df['Ambient_Temperature_C'] + 0.08 * df['Air_Fuel_Ratio'] + 0.005 * df['Engine_Speed_RPM']*df['Load_Percentage']  + np.random.normal(0, 2, num_samples)  # Interaction + noise\n",
        "\n",
        "    # Make efficiency dependent on fuel type with offsets\n",
        "    df['BioThermal_Efficiency'] = df.apply(lambda row: row['BioThermal_Efficiency'] + 2 if row['Fuel_Type'] == 'BioDiesel_20' else row['BioThermal_Efficiency'], axis=1)\n",
        "    df['BioThermal_Efficiency'] = df.apply(lambda row: row['BioThermal_Efficiency'] + 4 if row['Fuel_Type'] == 'BioDiesel_40' else row['BioThermal_Efficiency'], axis=1)\n",
        "\n",
        "    # Add missing values\n",
        "    for col in df.columns[:-1]:\n",
        "        mask = np.random.choice([True, False], num_samples, p=[0.1, 0.9])  # 10% missing values\n",
        "        df.loc[mask, col] = np.nan\n",
        "\n",
        "    #Introduce outliers\n",
        "    for col in df.select_dtypes(include='number').columns:\n",
        "        if col != \"BioThermal_Efficiency\":\n",
        "            df_col = df[col]\n",
        "            outliers_indicies = np.random.choice(df_col.index, int(0.01 * num_samples),replace = False)\n",
        "            df.loc[outliers_indicies,col] = df.loc[outliers_indicies,col] + (df.loc[outliers_indicies,col] * 1.5)\n",
        "    return df\n",
        "\n",
        "df = generate_data(temporal = True)\n",
        "\n",
        "# 2. Enhanced Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    #Missing values imputation\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    numeric_cols.remove('BioThermal_Efficiency')\n",
        "\n",
        "    imputer_knn = KNNImputer(n_neighbors=5)\n",
        "    df[numeric_cols] = imputer_knn.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Scaling and Transformation\n",
        "    X = df.drop('BioThermal_Efficiency', axis=1)\n",
        "    y = df['BioThermal_Efficiency']\n",
        "\n",
        "    #One hot encoding\n",
        "\n",
        "    categorical_cols = X.select_dtypes(include='object').columns\n",
        "    one_hot_encoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
        "\n",
        "    encoder_transformer = ColumnTransformer(\n",
        "        transformers = [\n",
        "            ('onehot', one_hot_encoder, categorical_cols)\n",
        "        ],\n",
        "        remainder = 'passthrough'\n",
        "    )\n",
        "    X_encoded = encoder_transformer.fit_transform(X)\n",
        "    column_names = encoder_transformer.get_feature_names_out()\n",
        "    X_encoded = pd.DataFrame(X_encoded, columns = column_names)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    #Robust scaling with QuantileTransformer\n",
        "    quantile_transformer = QuantileTransformer(output_distribution = 'normal', n_quantiles = 500)\n",
        "    X_train_scaled_quantile = quantile_transformer.fit_transform(X_train)\n",
        "    X_test_scaled_quantile = quantile_transformer.transform(X_test)\n",
        "\n",
        "    #Box-Cox or Yeo-Johnson\n",
        "    power_transform = PowerTransformer()\n",
        "    X_train_scaled_power = power_transform.fit_transform(X_train)\n",
        "    X_test_scaled_power = power_transform.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, X_train_scaled_quantile, X_test_scaled_quantile, X_train_scaled_power, X_test_scaled_power, y_train, y_test, scaler, encoder_transformer\n",
        "\n",
        "X_train, X_test, X_train_quantile, X_test_quantile, X_train_power, X_test_power, y_train, y_test, scaler, encoder = preprocess_data(df)\n",
        "\n",
        "\n",
        "# 3. Enhanced EDA\n",
        "def eda(df):\n",
        "    print(\"\\n--- Data Summary ---\")\n",
        "    print(df.describe(include='all'))\n",
        "\n",
        "    #Interactive scatter plot\n",
        "    fig = px.scatter_matrix(df.select_dtypes(include='number'),title=\"Interactive Scatter Matrix\")\n",
        "    fig.show()\n",
        "\n",
        "    print(\"\\n--- Correlation Analysis ---\")\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(df.corr(numeric_only = True), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "    plt.title(\"Correlation Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    #Skewness check\n",
        "    skew_values = df.select_dtypes(include='number').apply(lambda x: skew(x, nan_policy='omit'))\n",
        "    print(\"\\n--- Skewness of Numerical Features ---\")\n",
        "    print(skew_values)\n",
        "\n",
        "\n",
        "    print(\"\\n---Distribution of Numerical Features---\")\n",
        "    for col in df.select_dtypes(include='number').columns:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        sns.histplot(df[col].dropna(), kde=True)\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.show()\n",
        "\n",
        "    # Boxplot of Fuel Type vs. Efficiency\n",
        "    sns.boxplot(x = df['Fuel_Type'], y = df['BioThermal_Efficiency'])\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate Pearson, Spearman, Kendall correlations with target variable\n",
        "    target_col = \"BioThermal_Efficiency\"\n",
        "    print(\"\\n--- Correlation Coefficients with Target ---\")\n",
        "    for col in df.select_dtypes(include='number').columns:\n",
        "      if col != target_col:\n",
        "        pearson, _ = pearsonr(df[col].dropna(), df[target_col].dropna())\n",
        "        spearman, _ = spearmanr(df[col].dropna(), df[target_col].dropna())\n",
        "        kendall, _ = kendalltau(df[col].dropna(), df[target_col].dropna())\n",
        "\n",
        "        print(f\"{col}: Pearson = {pearson:.3f}, Spearman = {spearman:.3f}, Kendall = {kendall:.3f}\")\n",
        "\n",
        "    # Time series analysis\n",
        "    if 'time' in df.columns:\n",
        "       plt.figure(figsize=(10,6))\n",
        "       plt.plot(df['time'],df['BioThermal_Efficiency'])\n",
        "       plt.title('Time Series plot of Efficiency')\n",
        "       plt.xlabel('Time')\n",
        "       plt.ylabel(\"BioThermal_Efficiency\")\n",
        "       plt.show()\n",
        "\n",
        "\n",
        "eda(df)\n",
        "\n",
        "# 4. Enhanced Machine Learning Models\n",
        "def train_ml_models(X_train, y_train, X_test, y_test):\n",
        "    models = {\n",
        "        \"Linear Regression\": LinearRegression(),\n",
        "        \"Ridge Regression\": Ridge(),\n",
        "        \"Lasso Regression\": Lasso(),\n",
        "        \"Elastic Net Regression\": ElasticNet(),\n",
        "        \"Polynomial Regression\":  PolynomialFeatures(degree=2),\n",
        "        \"SVR\": SVR(),\n",
        "        \"Random Forest\": RandomForestRegressor(random_state=42),\n",
        "        \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
        "        \"AdaBoost\": AdaBoostRegressor(random_state=42),\n",
        "        \"Bagging Regressor\": BaggingRegressor(random_state=42)\n",
        "    }\n",
        "\n",
        "    tuned_models = {}\n",
        "    for name, model in models.items():\n",
        "\n",
        "        print(f\"\\n------{name} Training-------\")\n",
        "        if name == \"Polynomial Regression\":\n",
        "            poly = model\n",
        "            X_train_poly = poly.fit_transform(X_train)\n",
        "            X_test_poly = poly.transform(X_test)\n",
        "            lr_model = LinearRegression()\n",
        "            lr_model.fit(X_train_poly,y_train)\n",
        "            y_pred = lr_model.predict(X_test_poly)\n",
        "            tuned_models[name] = lr_model,poly\n",
        "\n",
        "        elif name == \"SVR\":\n",
        "            param_grid = {'kernel':['rbf', 'linear'],\n",
        "                           'C':[0.1, 1, 10],\n",
        "                           'gamma':['scale', 'auto', 0.1,1]\n",
        "                           }\n",
        "            grid = GridSearchCV(model, param_grid, cv = 3, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
        "            grid.fit(X_train, y_train)\n",
        "            best_model = grid.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            tuned_models[name] = best_model\n",
        "\n",
        "        elif name == \"Random Forest\":\n",
        "            param_grid = {'n_estimators':[100, 200, 500],\n",
        "                          'max_depth':[None, 5, 10],\n",
        "                          'min_samples_split':[2, 5, 10],\n",
        "                          'min_samples_leaf':[1, 2, 4],\n",
        "                          'max_features':['sqrt', 'log2', None]\n",
        "                          }\n",
        "            grid = RandomizedSearchCV(model, param_grid, cv = 3, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1)\n",
        "            grid.fit(X_train, y_train)\n",
        "            best_model = grid.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            tuned_models[name] = best_model\n",
        "\n",
        "        elif name == \"Gradient Boosting\":\n",
        "            param_grid = {'n_estimators':[100, 200, 500],\n",
        "                          'learning_rate':[0.01, 0.05, 0.1],\n",
        "                          'max_depth':[3,5,8],\n",
        "                          'min_samples_split':[2, 5, 10],\n",
        "                          'min_samples_leaf':[1, 2, 4],\n",
        "                          'max_features':['sqrt', 'log2', None]\n",
        "                          }\n",
        "            grid = RandomizedSearchCV(model, param_grid, cv = 3, scoring = 'neg_mean_squared_error', n_iter = 10, n_jobs = -1)\n",
        "            grid.fit(X_train, y_train)\n",
        "            best_model = grid.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            tuned_models[name] = best_model\n",
        "\n",
        "        elif name in [\"Ridge Regression\", \"Lasso Regression\", \"Elastic Net Regression\"]:\n",
        "            param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0], 'fit_intercept': [True, False]}\n",
        "            if name == \"Elastic Net Regression\":\n",
        "                param_grid[\"l1_ratio\"] = [0,0.25,0.5,0.75,1]\n",
        "            grid = GridSearchCV(model, param_grid, cv = 3, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
        "            grid.fit(X_train, y_train)\n",
        "            best_model = grid.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            tuned_models[name] = best_model\n",
        "\n",
        "        elif name in [\"AdaBoost\", \"Bagging Regressor\"]:\n",
        "            param_grid = {'n_estimators': [50, 100, 200],\n",
        "                         'learning_rate': [0.01, 0.1, 1.0]\n",
        "                        } if name == \"AdaBoost\" else {'n_estimators':[10, 50, 100]}\n",
        "\n",
        "            grid = RandomizedSearchCV(model, param_grid, cv = 3, scoring = 'neg_mean_squared_error', n_iter = 10 if name == 'AdaBoost' else 3, n_jobs = -1)\n",
        "            grid.fit(X_train, y_train)\n",
        "            best_model = grid.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            tuned_models[name] = best_model\n",
        "\n",
        "        else :\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            tuned_models[name] = model\n",
        "        evaluate_model(name,y_test,y_pred)\n",
        "    return tuned_models\n",
        "\n",
        "def evaluate_model(name, y_test, y_pred):\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        evs = explained_variance_score(y_test, y_pred)\n",
        "        max_err = max_error(y_test, y_pred)\n",
        "\n",
        "        print(f\"------{name} Evaluation-------\")\n",
        "        print(f\"MSE: {mse:.2f}\")\n",
        "        print(f\"RMSE: {rmse:.2f}\")\n",
        "        print(f\"MAE: {mae:.2f}\")\n",
        "        print(f\"R-squared: {r2:.2f}\")\n",
        "        print(f\"Explained Variance Score: {evs:.2f}\")\n",
        "        print(f\"Max Error: {max_err:.2f}\")\n",
        "\n",
        "tuned_ml_models = train_ml_models(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# 5. Enhanced Deep Learning Model\n",
        "def build_dl_model(input_shape,hp):\n",
        "        model = models.Sequential()\n",
        "\n",
        "        model.add(layers.Input(shape=(input_shape,)))\n",
        "\n",
        "        for i in range(hp.Int('num_layers', 2, 6)):\n",
        "            model.add(layers.Dense(units = hp.Int(f'units_{i}',32,256,step = 32),\n",
        "                                  activation = hp.Choice(f'activation_{i}', ['relu', 'tanh', 'selu'])))\n",
        "            model.add(LayerNormalization())\n",
        "            model.add(Dropout(rate = hp.Float(f'dropout_{i}', min_value = 0.0, max_value= 0.5, step=0.1)))\n",
        "\n",
        "        model.add(layers.Dense(1)) #Regression outout\n",
        "\n",
        "        optimizer_choice = hp.Choice('optimizer', ['adam', 'adamw'])\n",
        "        learning_rate = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
        "\n",
        "        if optimizer_choice == 'adam':\n",
        "            optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "        elif optimizer_choice == 'adamw':\n",
        "            optimizer = optimizers.AdamW(learning_rate=learning_rate)\n",
        "\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=[MeanSquaredError(), MeanAbsoluteError()])\n",
        "        return model\n",
        "\n",
        "def train_dl_model(X_train, y_train, X_test, y_test):\n",
        "    input_shape = X_train.shape[1]\n",
        "\n",
        "    def tuner_function(hp):\n",
        "        return build_dl_model(input_shape, hp)\n",
        "\n",
        "    tuner = RandomSearch(\n",
        "        tuner_function,\n",
        "        objective='val_mean_squared_error',\n",
        "        max_trials=10,\n",
        "        executions_per_trial=1,\n",
        "        directory = 'model_dir',\n",
        "        project_name = 'bio_thermal_tuner'\n",
        "    )\n",
        "\n",
        "    tuner.search(X_train,y_train,\n",
        "                validation_split=0.2,\n",
        "                epochs=100,\n",
        "                batch_size=32,\n",
        "                callbacks=[callbacks.EarlyStopping(monitor='val_loss',patience = 10, restore_best_weights = True)]\n",
        "        )\n",
        "\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "    best_model = tuner.hypermodel.build(best_hps)\n",
        "    history = best_model.fit(X_train,y_train,\n",
        "                            validation_split=0.2,\n",
        "                            epochs = 100,\n",
        "                            batch_size=32,\n",
        "                            verbose = 0,\n",
        "                             callbacks=[callbacks.EarlyStopping(monitor='val_loss',patience = 10, restore_best_weights = True)]\n",
        "                            )\n",
        "    y_pred = best_model.predict(X_test).flatten()\n",
        "    evaluate_model(\"Deep Learning MLP\",y_test,y_pred)\n",
        "    return best_model, history\n",
        "\n",
        "dl_model, dl_history = train_dl_model(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# 6. Generative Model\n",
        "def build_generator(latent_dim, n_features):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, input_dim=latent_dim),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.BatchNormalization(momentum = 0.8),\n",
        "        layers.Dense(256),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.BatchNormalization(momentum = 0.8),\n",
        "        layers.Dense(n_features, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_discriminator(n_features):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(256, input_dim = n_features),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(128),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_gan(df, epochs=5000, batch_size=32, latent_dim=10):\n",
        "\n",
        "    X = df.drop('BioThermal_Efficiency', axis=1)\n",
        "\n",
        "    #One hot encoding\n",
        "    categorical_cols = X.select_dtypes(include='object').columns\n",
        "    one_hot_encoder = OneHotEncoder(sparse_output = False, handle_unknown = 'ignore')\n",
        "\n",
        "    encoder_transformer = ColumnTransformer(\n",
        "        transformers = [\n",
        "            ('onehot', one_hot_encoder, categorical_cols)\n",
        "        ],\n",
        "        remainder = 'passthrough'\n",
        "    )\n",
        "    X_encoded = encoder_transformer.fit_transform(X)\n",
        "    column_names = encoder_transformer.get_feature_names_out()\n",
        "    X_encoded = pd.DataFrame(X_encoded, columns = column_names)\n",
        "    X_encoded = X_encoded.values.astype('float32')\n",
        "\n",
        "    n_features = X_encoded.shape[1]\n",
        "    generator = build_generator(latent_dim, n_features)\n",
        "    discriminator = build_discriminator(n_features)\n",
        "\n",
        "    discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "    generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer = discriminator_optimizer)\n",
        "    discriminator.trainable = False #Discriminator training will happen seperately\n",
        "\n",
        "    gan_input = layers.Input(shape = (latent_dim,))\n",
        "    gan_output = discriminator(generator(gan_input))\n",
        "    gan = models.Model(gan_input,gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer = generator_optimizer)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        idx = np.random.randint(0,X_encoded.shape[0],batch_size)\n",
        "        real_images = X_encoded[idx]\n",
        "\n",
        "        # Generate fake data\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        fake_images = generator.predict(noise)\n",
        "\n",
        "        # Train discriminator\n",
        "        discriminator.trainable = True\n",
        "        d_loss_real = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))\n",
        "        d_loss = 0.5 * np.add(d_loss_real,d_loss_fake)\n",
        "\n",
        "        # Train generator\n",
        "        discriminator.trainable = False\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch: {epoch}, D Loss: {d_loss:.3f}, G Loss: {g_loss:.3f}\")\n",
        "    return generator,encoder_transformer\n",
        "\n",
        "generator,encoder_transformer = train_gan(df)\n",
        "\n",
        "def generate_synthetic_samples(generator, latent_dim=10, num_samples=1000, encoder_transformer = None):\n",
        "        noise = np.random.normal(0, 1, (num_samples, latent_dim))\n",
        "        generated_data = generator.predict(noise)\n",
        "\n",
        "        if encoder_transformer:\n",
        "            generated_data = pd.DataFrame(generated_data, columns=encoder_transformer.get_feature_names_out())\n",
        "\n",
        "            original_cols = [col.split('__')[0] for col in encoder_transformer.get_feature_names_out()]\n",
        "            unique_cols = list(set(original_cols))\n",
        "\n",
        "            generated_df = pd.DataFrame(index = generated_data.index)\n",
        "\n",
        "            for col in unique_cols:\n",
        "                if col in original_cols:\n",
        "                    if col in df.columns:\n",
        "                       if  df[col].dtype == 'object':\n",
        "                            sub_df = generated_data[[c for c in generated_data.columns if c.startswith(col)]]\n",
        "                            generated_df[col] = sub_df.idxmax(axis=1).apply(lambda x : x.split('__')[1])\n",
        "                       else:\n",
        "                            generated_df[col] = generated_data[[c for c in generated_data.columns if c.startswith(col)]].sum(axis=1)\n",
        "\n",
        "            return generated_df\n",
        "        else:\n",
        "          return generated_data\n",
        "generated_samples = generate_synthetic_samples(generator,num_samples = 1000, encoder_transformer = encoder_transformer)\n",
        "\n",
        "\n",
        "# 7. Model Interpretability\n",
        "def interpret_model(model, X_train, X_test, y_train,y_test, explainer_type='shap'):\n",
        "    print(\"\\n--- Model Interpretability ---\")\n",
        "\n",
        "    if explainer_type == \"shap\":\n",
        "        explainer = shap.TreeExplainer(model) if isinstance(model, RandomForestRegressor) or isinstance(model, GradientBoostingRegressor) else shap.Explainer(model, X_train)\n",
        "        shap_values = explainer.shap_values(X_test)\n",
        "        shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "\n",
        "    elif explainer_type == \"lime\":\n",
        "         explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "             X_train, feature_names = X_train.columns, class_names = ['efficiency'], mode = 'regression'\n",
        "         )\n",
        "         index = np.random.choice(X_test.index)\n",
        "         exp = explainer.explain_instance(X_test.loc[index], model.predict, num_features = 10)\n",
        "         exp.show_in_notebook(show_table = True)\n",
        "\n",
        "    # if explainer_type == \"dl\":\n",
        "    #     explainer = shap.DeepExplainer(model, X_train)\n",
        "    #     shap_values = explainer.shap_values(X_test)\n",
        "    #     shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "\n",
        "for name, model in tuned_ml_models.items():\n",
        "    if name == \"Polynomial Regression\":\n",
        "        model, poly = model\n",
        "        X_train_poly = poly.transform(X_train)\n",
        "        X_test_poly = poly.transform(X_test)\n",
        "        interpret_model(model, X_train_poly, X_test_poly, y_train, y_test)\n",
        "    else:\n",
        "        interpret_model(model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "#Interpret DL model\n",
        "#interpret_model(dl_model, X_train, X_test, y_train, y_test, explainer_type = \"dl\")\n",
        "\n",
        "# 8. Discussion on Generative AI and Model Combination\n",
        "def generative_ai_discussion(generated_samples):\n",
        "    print(\"\\n--- Generative AI Discussion ---\")\n",
        "    print(\"GANs have been used to generate synthetic data with similar statistical properties as our training data.\")\n",
        "    print(\"Here's how this generated data can be used:\")\n",
        "    print(\" 1. Data Augmentation: The generated data can be combined with the real dataset to expand the dataset's scope, potentially leading to more robust regression model.\")\n",
        "    print(\" 2. Exploration of New Parameter Spaces: The GAN can help explore areas of engine parameters that we haven't seen in our data.\")\n",
        "    print(\" 3. Model Combination: We can also use the generated data to train a separate regression model and then average or ensemble that model with our models trained on original data.\")\n",
        "    print(\"The GAN is not for direct prediction but to augment or aid in the exploration of parameter space\")\n",
        "\n",
        "generative_ai_discussion(generated_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "amjalcz5_wB3",
        "outputId": "24fdd09d-c89a-4ef5-fca3-70fd56eddefe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9 Complete [00h 00m 33s]\n",
            "val_mean_squared_error: 20003.474609375\n",
            "\n",
            "Best val_mean_squared_error So Far: 17638.048828125\n",
            "Total elapsed time: 00h 05m 58s\n",
            "\n",
            "Search: Running Trial #10\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "3                 |2                 |num_layers\n",
            "160               |160               |units_0\n",
            "relu              |selu              |activation_0\n",
            "0.1               |0                 |dropout_0\n",
            "224               |128               |units_1\n",
            "relu              |relu              |activation_1\n",
            "0                 |0.3               |dropout_1\n",
            "adamw             |adamw             |optimizer\n",
            "2.9065e-05        |0.00080078        |learning_rate\n",
            "96                |224               |units_2\n",
            "relu              |selu              |activation_2\n",
            "0.3               |0.4               |dropout_2\n",
            "64                |256               |units_3\n",
            "relu              |selu              |activation_3\n",
            "0                 |0.2               |dropout_3\n",
            "160               |256               |units_4\n",
            "tanh              |tanh              |activation_4\n",
            "0.4               |0                 |dropout_4\n",
            "256               |128               |units_5\n",
            "relu              |relu              |activation_5\n",
            "0                 |0.3               |dropout_5\n",
            "\n",
            "Epoch 1/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 429887.1875 - mean_absolute_error: 563.7155 - mean_squared_error: 429887.1875 - val_loss: 454005.9062 - val_mean_absolute_error: 589.7022 - val_mean_squared_error: 454005.9062\n",
            "Epoch 2/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 450183.1250 - mean_absolute_error: 581.3592 - mean_squared_error: 450183.1250 - val_loss: 452537.8438 - val_mean_absolute_error: 588.6447 - val_mean_squared_error: 452537.8438\n",
            "Epoch 3/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 450526.7188 - mean_absolute_error: 576.3636 - mean_squared_error: 450526.7188 - val_loss: 451236.2812 - val_mean_absolute_error: 587.6854 - val_mean_squared_error: 451236.2812\n",
            "Epoch 4/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 458810.7500 - mean_absolute_error: 582.9830 - mean_squared_error: 458810.7500 - val_loss: 450109.2188 - val_mean_absolute_error: 586.8154 - val_mean_squared_error: 450109.2188\n",
            "Epoch 5/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 446177.9688 - mean_absolute_error: 576.7773 - mean_squared_error: 446177.9688 - val_loss: 449127.4375 - val_mean_absolute_error: 586.0287 - val_mean_squared_error: 449127.4375\n",
            "Epoch 6/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 447419.8125 - mean_absolute_error: 576.3986 - mean_squared_error: 447419.8125 - val_loss: 448250.7812 - val_mean_absolute_error: 585.3073 - val_mean_squared_error: 448250.7812\n",
            "Epoch 7/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 448755.8125 - mean_absolute_error: 575.4264 - mean_squared_error: 448755.8125 - val_loss: 447442.0625 - val_mean_absolute_error: 584.6254 - val_mean_squared_error: 447442.0625\n",
            "Epoch 8/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 452932.3750 - mean_absolute_error: 580.9547 - mean_squared_error: 452932.3750 - val_loss: 446675.3125 - val_mean_absolute_error: 583.9652 - val_mean_squared_error: 446675.3125\n",
            "Epoch 9/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 436493.9375 - mean_absolute_error: 563.8068 - mean_squared_error: 436493.9375 - val_loss: 445942.3125 - val_mean_absolute_error: 583.3284 - val_mean_squared_error: 445942.3125\n",
            "Epoch 10/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 457108.9688 - mean_absolute_error: 582.6870 - mean_squared_error: 457108.9688 - val_loss: 445227.6562 - val_mean_absolute_error: 582.7126 - val_mean_squared_error: 445227.6562\n",
            "Epoch 11/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 448394.4062 - mean_absolute_error: 579.9370 - mean_squared_error: 448394.4062 - val_loss: 444529.4375 - val_mean_absolute_error: 582.1070 - val_mean_squared_error: 444529.4375\n",
            "Epoch 12/100\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 448365.8438 - mean_absolute_error: 576.1641 - mean_squared_error: 448365.8438 - val_loss: 443830.7812 - val_mean_absolute_error: 581.4993 - val_mean_squared_error: 443830.7812\n",
            "Epoch 13/100\n",
            "\u001b[1m47/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 462943.1250 - mean_absolute_error: 588.8845 - mean_squared_error: 462943.1250"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-77a86de09fb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m \u001b[0mdl_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dl_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;31m# 6. Generative Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-77a86de09fb4>\u001b[0m in \u001b[0;36mtrain_dl_model\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     tuner.search(X_train,y_train,\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Save the build config for model loading later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/callback_list.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner_utils.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbetter_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner_utils.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         )\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Remove temporary saved model files on non-chief workers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         backend.io.remove_temp_dir_with_filepath(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mvia\u001b[0m \u001b[0man\u001b[0m \u001b[0minteractive\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msaving_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtraceback_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(model, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproceed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0msaving_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36msave_weights_only\u001b[0;34m(model, filepath, objects_to_skip)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mvisited_saveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m     _save_state(\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0mweights_store\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_store\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_save_state\u001b[0;34m(saveable, weights_store, assets_store, inner_path, visited_saveables)\u001b[0m\n\u001b[1;32m    680\u001b[0m             )\n\u001b[1;32m    681\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             _save_container_state(\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0mchild_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                 \u001b[0mweights_store\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_save_container_state\u001b[0;34m(container, weights_store, assets_store, inner_path, visited_saveables)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0mused_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             _save_state(\n\u001b[0m\u001b[1;32m    800\u001b[0m                 \u001b[0msaveable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m                 \u001b[0mweights_store\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_save_state\u001b[0;34m(saveable, weights_store, assets_store, inner_path, visited_saveables)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"save_own_variables\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mweights_store\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m         \u001b[0msaveable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_own_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    664\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaveable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"save_assets\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0massets_store\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0msaveable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_assets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massets_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py\u001b[0m in \u001b[0;36msave_own_variables\u001b[0;34m(self, store)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quantization_mode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mstore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_own_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    996\u001b[0m             \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bfloat16\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/base.py\u001b[0m in \u001b[0;36m_e\u001b[0;34m(self, name, lcpl)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mcoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSET_ASCII\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime keras-tuner scikit-learn plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3d0vo-n_wz_",
        "outputId": "5c785723-bed2-4cb3-98d9-4ed5937eba51"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/275.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2024.12.12)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=8e240cac80b7de5795ae5839c9d786091ea5c6cba5eaf45495da402e639e6fc0\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: kt-legacy, lime, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5 lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5Rp7YdhRAKo2"
      }
    }
  ]
}
