{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQjNKsWnceB8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.init as init\n",
        "import os\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# --- Step 0: Create Sample Data ---\n",
        "print(\"=\" * 70)\n",
        "print(\"Step 0: Creating Sample Data\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def create_sample_data(filename='news_summary.csv', num_samples=5000):\n",
        "    \"\"\"Creates a sample CSV file with dummy text and summaries\"\"\"\n",
        "    texts = [\n",
        "        \"This is a sample article about the weather. It's sunny today with a high of 25 degrees.\",\n",
        "        \"The stock market had a good day with all the major indices up more than one percent.\",\n",
        "        \"Scientists have made a new discovery in cancer research that could lead to new treatments.\",\n",
        "        \"The football team won their game last night in a close match.\",\n",
        "        \"There's a new movie coming out that is generating a lot of buzz.\",\n",
        "        \"Global warming is a serious threat and needs to be addressed by governments.\",\n",
        "        \"The new study has shown that most people enjoy pizza.\",\n",
        "        \"The local news has announced there will be heavy snow tomorrow.\",\n",
        "        \"A new software has been released with significant bug fixes.\",\n",
        "        \"The national park has been shut down due to a hurricane.\"\n",
        "    ]\n",
        "    summaries = [\n",
        "        \"Sunny weather expected today.\",\n",
        "        \"Stock market jumps.\",\n",
        "        \"Cancer research breakthrough.\",\n",
        "        \"Football team wins close game.\",\n",
        "        \"New movie generating buzz.\",\n",
        "        \"Global warming a serious issue.\",\n",
        "        \"Study shows people like pizza.\",\n",
        "        \"Heavy snow expected tomorrow.\",\n",
        "        \"New software with bug fixes.\",\n",
        "        \"National park shut down for hurricane.\"\n",
        "    ]\n",
        "\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        text_index = random.randint(0, len(texts)-1)\n",
        "        text = texts[text_index]\n",
        "        summary = summaries[text_index]\n",
        "        data.append({'text': text, 'summary': summary})\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"  Sample data created and saved to '{filename}'.\")\n",
        "    return filename\n",
        "\n",
        "# Create sample data\n",
        "file_path = create_sample_data() # this will create data\n",
        "\n",
        "\n",
        "# --- Step 1: Data Preprocessing ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Step 1: Data Preprocessing\")\n",
        "print(\"=\" * 70)\n",
        "def load_data(filepath, sample_size=None):\n",
        "    \"\"\"\n",
        "    Loads data from CSV, with optional sampling and columns for text and summary\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(filepath)\n",
        "    if sample_size:\n",
        "        df = df.sample(n=sample_size, random_state=SEED)\n",
        "\n",
        "    # Ensure consistent column names for text and summary\n",
        "    if \"text\" not in df.columns or \"summary\" not in df.columns:\n",
        "        if \"article\" in df.columns and \"highlights\" in df.columns:\n",
        "            df = df.rename(columns={\"article\": \"text\", \"highlights\": \"summary\"})\n",
        "        elif df.shape[1] >= 2: # if it has only two columns, assume its text and summary\n",
        "            df = df.rename(columns={df.columns[0]:\"text\", df.columns[1]:\"summary\"})\n",
        "        else:\n",
        "             raise ValueError(\"Dataframe must have at least two columns, or columns labeled 'text', 'summary', 'article', 'highlights'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the text by lowercasing, removing special characters, and tokenizing.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words] # removed stop words from the tokenized sequence\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def build_vocabulary(texts, min_freq=2):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary from the preprocessed tokens.\n",
        "    \"\"\"\n",
        "    all_tokens = [token for text in texts for token in text]\n",
        "    token_counts = Counter(all_tokens)\n",
        "    vocabulary = [token for token, count in token_counts.items() if count >= min_freq] #only taking the tokens that appear more than min_freq\n",
        "    vocabulary.insert(0, '<pad>')\n",
        "    vocabulary.insert(1, '<unk>') # Unknown tokens will be indexed to 1\n",
        "    return {token: idx for idx, token in enumerate(vocabulary)}\n",
        "\n",
        "\n",
        "def numericalize_text(tokens, vocabulary):\n",
        "  \"\"\"Convert tokenized text to numerical sequences using the vocabulary.\n",
        "  if the token isn't in the vocabulary, then it is replaced with the unknown token\n",
        "  \"\"\"\n",
        "  return [vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "\n",
        "def preprocess_data(df, min_freq=2):\n",
        "    \"\"\"\n",
        "    Applies the preprocessing steps to the dataframe.\n",
        "    \"\"\"\n",
        "    df['text_tokens'] = df['text'].apply(preprocess_text)\n",
        "    df['summary_tokens'] = df['summary'].apply(preprocess_text)\n",
        "\n",
        "    all_text_tokens =  df['text_tokens'].tolist() + df['summary_tokens'].tolist() # combine both for building the vocab\n",
        "    vocabulary = build_vocabulary([token for tokens in all_text_tokens for token in tokens], min_freq)\n",
        "\n",
        "    df['text_numerical'] = df['text_tokens'].apply(lambda tokens: numericalize_text(tokens, vocabulary))\n",
        "    df['summary_numerical'] = df['summary_tokens'].apply(lambda tokens: numericalize_text(tokens, vocabulary))\n",
        "    return df, vocabulary\n",
        "\n",
        "# Load and preprocess data\n",
        "df = load_data(file_path, sample_size=5000) # changed filename and sample size here\n",
        "processed_df, vocabulary = preprocess_data(df, min_freq = 3) # added the min_freq here, reducing it from 5 to 3 for increased vocabulary size\n",
        "vocab_size = len(vocabulary)\n",
        "print(\"   Vocabulary size:\", vocab_size)\n",
        "print(\"   Data Preprocessing Complete.\")\n",
        "print(processed_df.head())\n",
        "\n",
        "# --- Step 2: Dataset and DataLoader ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Step 2: Dataset and DataLoader\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "class TextSummaryDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for text summarization\n",
        "    \"\"\"\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_numerical = self.df['text_numerical'].iloc[idx]\n",
        "        summary_numerical = self.df['summary_numerical'].iloc[idx]\n",
        "        return torch.tensor(text_numerical), torch.tensor(summary_numerical)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Pads sequences to the maximum length in the batch\n",
        "    \"\"\"\n",
        "    text_sequences = [item[0] for item in batch]\n",
        "    summary_sequences = [item[1] for item in batch]\n",
        "\n",
        "    padded_text_seqs = pad_sequence(text_sequences, batch_first=True, padding_value=0)\n",
        "    padded_summary_seqs = pad_sequence(summary_sequences, batch_first=True, padding_value=0) # uses the pad token in index 0 as the padding value\n",
        "\n",
        "    return padded_text_seqs, padded_summary_seqs\n",
        "\n",
        "# Split dataset into train/val\n",
        "train_df, val_df = train_test_split(processed_df, test_size=0.2, random_state=SEED)\n",
        "\n",
        "train_dataset = TextSummaryDataset(train_df)\n",
        "val_dataset = TextSummaryDataset(val_df)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "print(\"  Dataset and DataLoader setup complete\")\n",
        "\n",
        "\n",
        "# --- Step 3: Model Architecture ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Step 3: Model Architecture\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Positional Encoding\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Adds the positional encoding to the input\n",
        "        \"\"\"\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of Multi-Head Attention\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "      \"\"\"Calculates scaled dot-product attention\"\"\"\n",
        "      attn_scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.d_head)\n",
        "      if mask is not None:\n",
        "\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "      attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "      output = torch.matmul(attn_probs, v)\n",
        "      return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"Splits the input into heads\"\"\"\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        \"\"\"Combines the heads back together\"\"\"\n",
        "        batch_size, _, seq_length, d_head = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "      \"\"\"Forward pass for multi head attention\"\"\"\n",
        "      Q = self.W_q(q)\n",
        "      K = self.W_k(k)\n",
        "      V = self.W_v(v)\n",
        "\n",
        "      Q_split = self.split_heads(Q)\n",
        "      K_split = self.split_heads(K)\n",
        "      V_split = self.split_heads(V)\n",
        "\n",
        "      attn_output = self.scaled_dot_product_attention(Q_split, K_split, V_split, mask)\n",
        "      attn_output_combined = self.combine_heads(attn_output)\n",
        "      output = self.W_o(attn_output_combined)\n",
        "      return output\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Implementation of the feed-forward network\"\"\"\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass for Feed Forward Network\"\"\"\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"Encoder Layer of the Transformer\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "      \"\"\"Forward pass for the encoder layer\"\"\"\n",
        "      attn_output = self.mha(x,x,x,mask)\n",
        "      x = self.norm1(x + self.dropout(attn_output))\n",
        "      ff_output = self.ff(x)\n",
        "      x = self.norm2(x + self.dropout(ff_output))\n",
        "      return x\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"Decoder Layer of the Transformer\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.masked_mha = MultiHeadAttention(d_model, num_heads) #Masked Self Attention\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads) #Encoder Decoder Attention\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask = None, tgt_mask=None):\n",
        "      \"\"\"Forward pass for decoder layer\"\"\"\n",
        "      masked_attn_output = self.masked_mha(x,x,x,tgt_mask) # Masked self-attention\n",
        "      x = self.norm1(x + self.dropout(masked_attn_output))\n",
        "      attn_output = self.mha(x, enc_output, enc_output, src_mask)\n",
        "      x = self.norm2(x + self.dropout(attn_output))\n",
        "      ff_output = self.ff(x)\n",
        "      x = self.norm3(x + self.dropout(ff_output))\n",
        "      return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder module of the Transformer\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) #multiple encoder layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask=None):\n",
        "      \"\"\"Forward pass for Encoder\"\"\"\n",
        "      x = self.dropout(self.pos_encoder(self.embedding(x)))\n",
        "      for layer in self.layers:\n",
        "        x = layer(x,mask)\n",
        "      return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder Module of the Transformer\"\"\"\n",
        "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]) #Multiple decoder layers\n",
        "        self.fc = nn.Linear(d_model, vocab_size) # fully connected to produce output\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "      \"\"\"Forward pass for Decoder\"\"\"\n",
        "      x = self.dropout(self.pos_encoder(self.embedding(x)))\n",
        "      for layer in self.layers:\n",
        "          x = layer(x, enc_output, src_mask, tgt_mask)\n",
        "      x = self.fc(x)\n",
        "      return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"Full Transformer Model\"\"\"\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, max_len):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, d_ff, num_layers, dropout, max_len)\n",
        "\n",
        "        # Initialize weights with Xavier initialization\n",
        "        for p in self.parameters():\n",
        "           if p.dim() > 1:\n",
        "              init.xavier_uniform_(p)\n",
        "\n",
        "\n",
        "    def generate_mask(self, seq, pad_token=0):\n",
        "      \"\"\"Creates a mask for padding tokens\"\"\"\n",
        "      mask = (seq != pad_token).unsqueeze(1).unsqueeze(2).unsqueeze(3) # added an unsqueeze to shape as batch_size,1,seq_len,1\n",
        "      return mask\n",
        "\n",
        "    def generate_tgt_mask(self, seq):\n",
        "      \"\"\"Create a look ahead mask so the model doesn't look at future words.\"\"\"\n",
        "      seq_len = seq.size(1)\n",
        "      mask = torch.tril(torch.ones((seq_len, seq_len),dtype=torch.bool))\n",
        "      return mask\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "      \"\"\"Forward pass for the Transformer model\"\"\"\n",
        "      src_mask = self.generate_mask(src)\n",
        "      tgt_mask = self.generate_tgt_mask(tgt)\n",
        "      enc_output = self.encoder(src, src_mask)\n",
        "      output = self.decoder(tgt[:, :-1], enc_output, src_mask, tgt_mask) #remove last token for target since the target model will not predict that in training\n",
        "      return output\n",
        "\n",
        "# Model hyperparameters\n",
        "d_model = 256\n",
        "num_heads = 8\n",
        "d_ff = 1024\n",
        "num_layers = 4\n",
        "dropout = 0.1\n",
        "max_len = 512\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Initialize model and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(vocab_size, vocab_size, d_model, num_heads, d_ff, num_layers, dropout, max_len).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding tokens\n",
        "print(\"  Model Architecture Setup Complete\")\n",
        "\n",
        "# --- Step 4: Training ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Step 4: Training\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def train_step(model, iterator, optimizer, criterion, clip = 1):\n",
        "    \"\"\"Performs a single training step\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, tgt in iterator:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        output_reshape = output.contiguous().view(-1, output.shape[-1]) # reshaping to (batch*seq_len, vocab_size)\n",
        "        tgt_reshape = tgt[:, 1:].contiguous().view(-1) # reshaping to (batch*seq_len)\n",
        "\n",
        "        loss = criterion(output_reshape, tgt_reshape)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # prevents exploding gradient\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate_step(model, iterator, criterion):\n",
        "    \"\"\"Performs a single evaluation step\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in iterator:\n",
        "          src = src.to(device)\n",
        "          tgt = tgt.to(device)\n",
        "          output = model(src, tgt)\n",
        "\n",
        "          output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "          tgt_reshape = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "          loss = criterion(output_reshape, tgt_reshape)\n",
        "          epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs = 10, clip=1):\n",
        "  \"\"\"Main training loop\"\"\"\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      start_time = time.time()\n",
        "      train_loss = train_step(model, train_loader, optimizer, criterion, clip)\n",
        "      val_loss = evaluate_step(model, val_loader, criterion)\n",
        "      end_time = time.time()\n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(val_loss)\n",
        "      epoch_mins, epoch_secs = int((end_time - start_time) / 60), int((end_time - start_time) % 60)\n",
        "      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')\n",
        "  return train_losses, val_losses\n",
        "\n",
        "num_epochs = 10\n",
        "clip = 1\n",
        "train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, clip)\n",
        "\n",
        "# Plotting the losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(\"  Training Complete.\")\n",
        "\n",
        "# --- Step 5: Text Generation / Summarization ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Step 5: Text Generation / Summarization\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def generate_summary(model, src, vocabulary, max_len=150):\n",
        "    \"\"\"Generates a summary from the given input text\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src_tensor = torch.tensor(numericalize_text(preprocess_text(src), vocabulary)).unsqueeze(0).to(device)\n",
        "        src_mask = model.generate_mask(src_tensor)\n",
        "        enc_output = model.encoder(src_tensor, src_mask)\n",
        "\n",
        "        tgt_tokens = [vocabulary['<pad>']] #start with the padding token\n",
        "        for _ in range(max_len):\n",
        "            tgt_tensor = torch.tensor(tgt_tokens).unsqueeze(0).to(device)\n",
        "            tgt_mask = model.generate_tgt_mask(tgt_tensor)\n",
        "            output = model.decoder(tgt_tensor, enc_output, src_mask, tgt_mask)\n",
        "            output = output.argmax(dim=-1) #get the word with max probability\n",
        "            next_token = output[:,-1].item()\n",
        "            tgt_tokens.append(next_token)\n",
        "            if next_token == vocabulary['<pad>']:\n",
        "                break # if the next token is padding, then break\n",
        "\n",
        "        # remove pad token and translate numericalized to tokens\n",
        "        generated_tokens = [token for token, idx in vocabulary.items() if idx in tgt_tokens[1:]]\n",
        "        generated_summary = ' '.join(generated_tokens)\n",
        "    return generated_summary.strip()\n",
        "\n",
        "def show_random_summarization(df, model, vocabulary):\n",
        "  \"\"\"Generates the model's predicted summary against actual summary.\"\"\"\n",
        "  rand_index = random.randint(0,len(df)-1)\n",
        "  sample = df.iloc[rand_index]\n",
        "  src = sample['text']\n",
        "  tgt = sample['summary']\n",
        "  pred_tgt = generate_summary(model, src, vocabulary)\n",
        "  print(\"\\nExample Summarization\")\n",
        "  print(\"-----------------------\")\n",
        "  print(f\"  Original Text:\\n{src}\")\n",
        "  print(f\"\\n  Predicted Summary:\\n{pred_tgt}\")\n",
        "  print(f\"\\n  Actual Summary:\\n{tgt}\")\n",
        "\n",
        "show_random_summarization(val_df, model, vocabulary)\n",
        "\n",
        "print(\"Text Generation / Summarization Complete.\")\n",
        "\n",
        "# --- Step 6: Evaluation Metrics (BLEU) ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Step 6: Evaluation Metrics (BLEU)\")\n",
        "print(\"=\" * 70)\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "def calculate_bleu(model, val_df, vocabulary):\n",
        "  \"\"\"Calculates BLEU score for the entire validation set\"\"\"\n",
        "  actual_summaries = val_df['summary'].tolist()\n",
        "  predicted_summaries = []\n",
        "  for src in val_df['text'].tolist():\n",
        "    predicted_summaries.append(generate_summary(model, src, vocabulary))\n",
        "\n",
        "  actual_summaries_tokens = [preprocess_text(summary) for summary in actual_summaries]\n",
        "  predicted_summaries_tokens = [preprocess_text(summary) for summary in predicted_summaries]\n",
        "\n",
        "  smoothing = SmoothingFunction().method4\n",
        "  bleu_score = corpus_bleu(actual_summaries_tokens, predicted_summaries_tokens, smoothing_function=smoothing)\n",
        "  print(f\"  BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "calculate_bleu(model, val_df, vocabulary)\n",
        "\n",
        "print(\"Evaluation Complete.\")\n",
        "\n",
        "# --- Step 7: Model Export ---\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Step 7: Model Export\")\n",
        "print(\"=\" * 70)\n",
        "# Optional - saving model to file\n",
        "try:\n",
        "    torch.save(model.state_dict(), 'transformer_summarization_model.pth')\n",
        "    print(\"  Model successfully exported to transformer_summarization_model.pth\")\n",
        "except Exception as e:\n",
        "    print(\"Error saving model\", e)\n",
        "\n",
        "print(\"Data processing, training, and evaluation complete.\")"
      ],
      "metadata": {
        "id": "O2z6QKM23gWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}