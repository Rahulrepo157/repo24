{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74BaeLIgH1IG"
      },
      "outputs": [],
      "source": [
        "# feature_engineering.py\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import spacy\n",
        "import gensim\n",
        "import pandas as pd\n",
        "\n",
        "# Load SpaCy model once globally\n",
        "try:\n",
        "  nlp = spacy.load('en_core_web_lg')  # For word vectors\n",
        "except OSError:\n",
        "    print(\"Downloading SpaCy language model 'en_core_web_lg'...\")\n",
        "    spacy.cli.download(\"en_core_web_lg\")\n",
        "    nlp = spacy.load('en_core_web_lg')\n",
        "def create_tfidf_features(texts, max_features=5000, ngram_range=(1, 1)):\n",
        "    \"\"\"Creates TF-IDF features.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    return tfidf_matrix, vectorizer\n",
        "\n",
        "def create_average_word_embeddings(texts, embedding_dim=300):\n",
        "    \"\"\"Creates average word embedding features.\"\"\"\n",
        "    all_vectors = []\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        vectors = [token.vector for token in doc if not token.is_stop and token.has_vector]\n",
        "        if vectors:\n",
        "             average_vector = np.mean(vectors, axis=0)\n",
        "             all_vectors.append(average_vector)\n",
        "        else:\n",
        "            all_vectors.append(np.zeros(embedding_dim))  # or handle differently\n",
        "    return np.array(all_vectors)\n",
        "\n",
        "def create_word2vec_features(texts, embedding_dim=300):\n",
        "    \"\"\"Creates word2vec features.\"\"\"\n",
        "    # Create sentences for training\n",
        "    sentences = [text.split() for text in texts]\n",
        "\n",
        "    # Train Word2Vec model\n",
        "    model = gensim.models.Word2Vec(sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "    model.train(sentences, total_examples=len(sentences), epochs=10)\n",
        "\n",
        "    # Generate features\n",
        "    all_vectors = []\n",
        "    for text in texts:\n",
        "         doc = [token for token in text.split() ]\n",
        "         vectors = [model.wv[token] for token in doc if token in model.wv]\n",
        "         if vectors:\n",
        "             average_vector = np.mean(vectors, axis=0)\n",
        "             all_vectors.append(average_vector)\n",
        "         else:\n",
        "            all_vectors.append(np.zeros(embedding_dim))\n",
        "\n",
        "    return np.array(all_vectors), model\n",
        "\n",
        "def split_data(features, labels, test_size=0.2, random_state=42):\n",
        "    \"\"\"Splits the data into training and testing sets.\"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=test_size, random_state=random_state)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def main():\n",
        "   try:\n",
        "        data = pd.read_csv(\"reviews.csv\") # Assuming a 'text' column and 'sentiment' column\n",
        "        data.dropna(subset=[\"text\", \"sentiment\"], inplace=True)\n",
        "        from data_preprocessing import preprocess_dataframe\n",
        "        processed_df = preprocess_dataframe(data, \"text\")\n",
        "\n",
        "        texts = processed_df[\"processed_text\"].tolist()\n",
        "        labels = processed_df[\"sentiment\"].tolist()\n",
        "\n",
        "        # TF-IDF features\n",
        "        tfidf_matrix, tfidf_vectorizer = create_tfidf_features(texts)\n",
        "        print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n",
        "\n",
        "         # Split data for TF-IDF\n",
        "        X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = split_data(tfidf_matrix, labels)\n",
        "        print(\"TF-IDF Train Data Shape:\", X_train_tfidf.shape)\n",
        "        print(\"TF-IDF Test Data Shape:\", X_test_tfidf.shape)\n",
        "\n",
        "        # Average Word Embeddings\n",
        "        average_embeddings = create_average_word_embeddings(texts)\n",
        "        print(\"Average Word Embeddings shape:\", average_embeddings.shape)\n",
        "\n",
        "        # Split data for Average Word Embeddings\n",
        "        X_train_avg_emb, X_test_avg_emb, y_train_avg_emb, y_test_avg_emb = split_data(average_embeddings, labels)\n",
        "        print(\"Average Embeddings Train Data Shape:\", X_train_avg_emb.shape)\n",
        "        print(\"Average Embeddings Test Data Shape:\", X_test_avg_emb.shape)\n",
        "\n",
        "        # Word2Vec features\n",
        "        word2vec_embeddings, word2vec_model = create_word2vec_features(texts)\n",
        "        print(\"Word2Vec embeddings shape:\", word2vec_embeddings.shape)\n",
        "\n",
        "        # Split data for Word2Vec\n",
        "        X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = split_data(word2vec_embeddings, labels)\n",
        "        print(\"Word2Vec Train Data Shape:\", X_train_w2v.shape)\n",
        "        print(\"Word2Vec Test Data Shape:\", X_test_w2v.shape)\n",
        "   except FileNotFoundError:\n",
        "        print(\"Error: 'reviews.csv' not found. Please provide the data in a 'reviews.csv' file with 'text' and 'sentiment' columns.\")\n",
        "   except KeyError as e:\n",
        "        print(f\"Error: {e} column not found. Ensure your 'reviews.csv' has 'text' and 'sentiment' columns.\")\n",
        "   except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "   main()"
      ]
    }
  ]
}